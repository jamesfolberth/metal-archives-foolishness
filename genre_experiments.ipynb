{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we make decent genre feature vectors?\n",
    "metal-archives already places bands in to black, death, doom/stoner/sludge, etc. genres.\n",
    "From that one can get all bands in the black metal genre, for instance.\n",
    "But that will lose the \"atmospheric\" modifier for \"atmostpheric black metal\", for example.\n",
    "So can we make a better word embedding or something from the genre text on the band's page?\n",
    "Then we can pick a band and find bands nearby in genre, perhaps sorted by popularity or something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections, string, itertools\n",
    "from pprint import pprint\n",
    "\n",
    "import sqlite3 as lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of tuples of (band_id, band, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filename = 'database.db'\n",
    "with lite.connect(db_filename) as connection:\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    genres_and_bands = cursor.execute('select band_id,band,genre from Bands').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(genres_and_bands))\n",
    "pprint(genres_and_bands[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = list({t[2] for t in genres_and_bands})\n",
    "unique_genres.sort()\n",
    "pprint(unique_genres[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The genre texts are pretty structured, but maybe not totally repeatable.  I'm seeing some patterns that I'd like to make sense of to get the modifiers correct.  I'm thinking I want to group **Technical** with **Death** for example.\n",
    "\n",
    "I'm seeing some patterns:\n",
    "* Clearly therese are comma separated lists\n",
    "* There are (early), (later), (mid), (Later) modifiers.\n",
    "* **Brutal/Technical Death Metal** should map to **Brutal Death Metal** and **Technical Death Metal**?\n",
    "* **Raw Black/Viking Metal** could be be **Raw Black Metal** and **Viking Metal** or **Raw Black Metal** and **Raw Viking Metal**.  The band in this example is Aasfresser; I'm guessing the former interpretation, but I'm no expert.\n",
    "* I want to remove **Metal** and **Rock**, but **Alternative Metal** shouldn't be the same as **Alternative Rock**.  So maybe it's better to keep these \"0th level\" genres and attach all the modifiers to them a la **Alternative Metal** and **Alternative Rock**.  But then that screws up if the genre is just **Funeral Doom**, not **Funeral Doom Metal**.  Hmm...\n",
    "\n",
    "This is gonna piss people off, but I'm going to define supergenres = metal, rock, punk, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in parens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_parens_regex = re.compile(r'\\(.*?\\)')\n",
    "in_parens = set()\n",
    "for genre in unique_genres:\n",
    "    for match in re.findall(in_parens_regex, genre):\n",
    "        in_parens.add(match)\n",
    "\n",
    "#pprint(in_parens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably just going to drop these.  But when when tokenizing or TF/IDF-ing (if we go that route), I don't want to double-count something like Prog Death (early), Prog Death/Black Metal(later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Black/Viking Metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_black_viking = [t for t in genres_and_bands if t[2].find('Raw Black/Viking Metal') != -1]\n",
    "raw_black_viking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allgrams(text):\n",
    "    tokens = text.split(' ')\n",
    "    return tokens\n",
    "\n",
    "def flatten(iterable):\n",
    "    return [item for subiterable in iterable for item in subiterable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreTokenizer(object):\n",
    "    \"\"\"\n",
    "    Try to tokenize those pesky genre texts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.whitespace_regex = re.compile(r'\\s+')\n",
    "        self.paren_regex = re.compile(r'\\(.*?\\)')\n",
    "        self.split_regex = re.compile(r\"\"\"[/]+\"\"\")\n",
    "        \n",
    "        self.pre_split_replace = {\" 'n' \": \"'n'\",\n",
    "                                  \" n' \": \"'n'\",\n",
    "                                 }\n",
    "        \n",
    "        # this is stupid!  let the data speak for itself!\n",
    "        self.simple_super_genres = frozenset([\n",
    "                                        'acoustic',\n",
    "                                        'alternative',\n",
    "                                        'ambient',\n",
    "                                        'aor',\n",
    "                                        'classical',\n",
    "                                        'crossover',\n",
    "                                        'crust',\n",
    "                                        'djent',\n",
    "                                        'drone',\n",
    "                                        'doom',\n",
    "                                        'downtempo',\n",
    "                                        'electronic',\n",
    "                                        'electronica',\n",
    "                                        'electronics',\n",
    "                                        'experimental',\n",
    "                                        'folk',\n",
    "                                        'funk',\n",
    "                                        'gothic',\n",
    "                                        'grunge',\n",
    "                                        'industrial',\n",
    "                                        'medieval',\n",
    "                                        'metal',\n",
    "                                        'noise',\n",
    "                                        'psybient',\n",
    "                                        'psychedelic',\n",
    "                                        'punk',\n",
    "                                        'rock',\n",
    "                                        'synth',\n",
    "                                        'trance',\n",
    "                                       ])\n",
    "        self.compound_super_genre_prefix = frozenset(['neo', 'post', 'synth',])\n",
    "        self.compound_super_genre_suffix = frozenset(['core', 'gaze', 'wave',\n",
    "                                                     ])\n",
    "\n",
    "        self.pre_split_words = frozenset([\"rock 'n' roll\",\n",
    "                                          \"death 'n' roll\",\n",
    "                                          \"black 'n' roll\",\n",
    "                                          \"thrash 'n' roll\",\n",
    "                                          'drum and bass',\n",
    "                                          'a cappella',\n",
    "                                          'middle eastern',\n",
    "                                          'middle-eastern',\n",
    "                                          'avant-garde',\n",
    "                                          'j-rock',\n",
    "                                          'd-beat',\n",
    "                                          'new age',\n",
    "                                          'nu-metal',\n",
    "                                         ])\n",
    "\n",
    "        self.drop_tokens = frozenset([None,'',\n",
    "                            'influences',\n",
    "                           'elements',\n",
    "                           'of',\n",
    "                           'and'])\n",
    "        \n",
    "        self.token_map = {'blackened': 'black',\n",
    "                          'middle eastern': 'middle-eastern',\n",
    "                          'neoclassic': 'neoclassical',\n",
    "                          'operatic': 'opera',\n",
    "                          'drum and bass': 'drum-and-bass',\n",
    "                         }\n",
    "        \n",
    "        self.bad_tokens = frozenset([None,'','age',\n",
    "                                    ])\n",
    "        \n",
    "        self.split_these_tokens = ['core', 'noise', 'grind', 'synth', 'wave',]\n",
    "        \n",
    "    def tokenize(self, genre_text):\n",
    "        genre_text = self.normalizeWhitespace(genre_text).lower()\n",
    "        genre_text = self.removeParens(genre_text)\n",
    "        texts = genre_text.split(',')\n",
    "        \n",
    "        # Check for any remaining \"bad\" characters\n",
    "        for text in texts:\n",
    "            if '(' in text or ')' in text or ',' in text:\n",
    "                print(genre_text, texts)\n",
    "                raise RuntimeError('bug')\n",
    "        \n",
    "        # do the messy split split\n",
    "        tokens = [token for text in texts for token in self.split2(text)]\n",
    "        \n",
    "        # Check for weird tokens\n",
    "        for bad_token in self.bad_tokens:\n",
    "            if bad_token in tokens or any(map(lambda token: len(token)==1, tokens)):\n",
    "                print(genre_text, tokens)\n",
    "                raise RuntimeError('bug')\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def normalizeWhitespace(self, text):\n",
    "        \"\"\"\n",
    "        Remove leading and trailing whitespaces.  Use only one space between non-whitespace characters.\n",
    "        \"\"\"\n",
    "        return re.sub(self.whitespace_regex, ' ', text).strip().rstrip()\n",
    "    \n",
    "    def removeParens(self, text):\n",
    "        \"\"\"\n",
    "        Drop the stuff in parentheses.\n",
    "        \"\"\"\n",
    "        return re.sub(self.paren_regex, '', text).strip().rstrip()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def is_super_genre(self, token):\n",
    "        if token in self.simple_super_genres:\n",
    "            return True\n",
    "        \n",
    "        for prefix in self.compound_super_genre_prefix:\n",
    "            if token[0:len(prefix)] == prefix:\n",
    "                return True\n",
    "            \n",
    "        for ending in self.compound_super_genre_suffix:\n",
    "            if token[-len(ending):] == ending:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        Do the split.  This is gonna be gross...\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        \n",
    "        text = self.normalizeWhitespace(text)\n",
    "        \n",
    "        # Do some replacements before any further tokenization\n",
    "        for pat,sub in self.pre_split_replace.items():\n",
    "            text = text.replace(pat,sub)\n",
    "        \n",
    "        # Grab these special cases first and remove them from the text\n",
    "        pre_split_tokens = []\n",
    "        for word in self.pre_split_words:\n",
    "            if word in text:\n",
    "                text = text.replace(word, ' ')\n",
    "                pre_split_tokens.append(word)\n",
    "                \n",
    "        # Deal with modifiers like 'atmospheric black/folk metal'\n",
    "        # We want 'atmospheric', 'atmospheric black', 'atmospheric black metal', 'black metal',\n",
    "        # and 'folk metal' as tokens.\n",
    "        splits = flatten(text.split(' ') for text in re.split('(/)', text)) # keep the / in the list\n",
    "        \n",
    "        # Drop certain tokens\n",
    "        splits = [token for token in splits if token not in self.drop_tokens]\n",
    "        \n",
    "        if splits and splits[0] == '/': # weird edge case in the data\n",
    "            splits.pop(0)\n",
    "        \n",
    "        #print(text)\n",
    "        #print(splits)\n",
    "        \n",
    "        # Starting from the end, find a super genre and look ahead until the next super genre.\n",
    "        # Then that slice is a \"structured token\".  Gotta handle what the slash means too.\n",
    "        # Some genre texts have \"modified super_genre with super_genre influences\"; those super_genre\n",
    "        # influences get counted as a separate super_genre token.\n",
    "        structured_tokens = []\n",
    "        i = len(splits)-1\n",
    "        while i >= 0:\n",
    "            token = splits[i]\n",
    "            \n",
    "            if token == '/': # weird edge case in the data, not this algorithm\n",
    "                i -= 1\n",
    "                continue\n",
    "                \n",
    "            #print(token)\n",
    "            if not self.is_super_genre(token):\n",
    "                print(text)\n",
    "                print(splits)\n",
    "                raise RuntimeError('token={} is not a super genre'.format(token))\n",
    "                #print('skipping apparent super genre {}'.format(token))\n",
    "            \n",
    "            iend = i\n",
    "            istart = i-1\n",
    "            have_slash = False\n",
    "            slash_is_modifier = False\n",
    "            while istart >= 0:\n",
    "                if splits[istart] == '/':\n",
    "                    have_slash = True\n",
    "                    if istart > 0:\n",
    "                        if self.is_super_genre(splits[istart-1]):\n",
    "                            # slash separates super genres\n",
    "                            #print('slash separates super genres')\n",
    "                            structured_tokens.append(splits[istart+1:iend+1])\n",
    "                            i = istart - 1\n",
    "                            break\n",
    "                        else:\n",
    "                            # slash separates modifiers\n",
    "                            #print('slash separates modifiers')\n",
    "                            slash_is_modifier = True\n",
    "                            istart -= 1\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(text)\n",
    "                        print(splits)\n",
    "                        raise RuntimeError('bug')\n",
    "                else:\n",
    "                    if self.is_super_genre(splits[istart]) and slash_is_modifier:\n",
    "                        structured_tokens.append(splits[istart+1:iend+1])\n",
    "                        i = istart\n",
    "                        break\n",
    "                    else:\n",
    "                        istart -= 1\n",
    "                \n",
    "                #istart -= 1\n",
    "            else:\n",
    "                structured_tokens.append(splits[0:iend+1])\n",
    "                i = istart\n",
    "        \n",
    "        print('text =', repr(text), ' -> structured_tokens =', structured_tokens)\n",
    "        \n",
    "        return tokens\n",
    "    #############################\n",
    "        \n",
    "        split_tokens = re.split(self.split_regex, text)\n",
    "        \n",
    "        for token in itertools.chain(pre_split_tokens, split_tokens):\n",
    "            if token in self.drop_tokens:\n",
    "                continue\n",
    "            \n",
    "            if token in self.token_map:\n",
    "                token = self.token_map[token]\n",
    "            \n",
    "            tokens.append(token)\n",
    "            \n",
    "            #for base in self.split_these_tokens:\n",
    "            #    if base in token:\n",
    "            #        print('stuff')\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "        \n",
    "    def split2(self, text):\n",
    "        \"\"\"\n",
    "        Do the split.  This is gonna be gross...\n",
    "        \n",
    "        This assumes the following structure\n",
    "        \n",
    "        modifier modifier/modifier super_genre with modifier influences\n",
    "        \"\"\"\n",
    "        # Do some replacements before any further tokenization\n",
    "        for pat,sub in self.pre_split_replace.items():\n",
    "            text = text.replace(pat,sub)\n",
    "            \n",
    "        # Deal with modifiers like 'atmospheric black/folk metal'\n",
    "        # We want 'atmospheric', 'atmospheric black', 'atmospheric black metal', 'black metal',\n",
    "        # and 'folk metal' as tokens.\n",
    "        splits = flatten(re.split(r'( )', text) for text in re.split(r'(/)', text)) # keep the / in the list\n",
    "        \n",
    "        # Drop certain tokens\n",
    "        splits = [token for token in splits if token not in self.drop_tokens]\n",
    "        \n",
    "        if splits and (splits[0] == '/' or splits[0] == ' '): # weird edge case in the data\n",
    "            splits.pop(0)\n",
    "        if splits and (splits[-1] == '/' or splits[-1] == ' '): # weird edge case in the data\n",
    "            splits.pop(-1)\n",
    "        \n",
    "        print('text =', text)\n",
    "        print('splits =', splits)\n",
    "        \n",
    "        extra_modifiers = []\n",
    "        if 'with' in splits:\n",
    "            ind = splits.index('with')\n",
    "            extra_modifiers.extend(splits[ind+1:])\n",
    "            splits = splits[0:ind]\n",
    "            \n",
    "            extra_modifiers = [token for token in extra_modifiers if token != ' ']\n",
    "            print('extra_modifiers =', extra_modifiers)\n",
    "            print('splits =', splits)\n",
    "        \n",
    "        super_genres = frozenset(['rock', 'metal'])\n",
    "        \n",
    "        def splitter(tokens, splits, i):\n",
    "            # token is assumed to be a super genre here\n",
    "            token = splits[i]\n",
    "            \n",
    "            # but sometimes the data entry is wrong...\n",
    "            if token == ' ' or token == '/':\n",
    "                print('Bug in data entry')\n",
    "                splitter(tokens, splits, i-1)\n",
    "                return\n",
    "                \n",
    "            #print(\"token={} isn't in super_genres set\".format(token))\n",
    "                \n",
    "            \n",
    "            if i > 0:\n",
    "                if splits[i-1] == ' ':\n",
    "                    # there are modifiers; collect them all\n",
    "                    modifiers = [[]]\n",
    "                    i1 = i-2\n",
    "                    while i1 >= 0:\n",
    "                        mod = splits[i1]\n",
    "                        \n",
    "                        if mod == ' ' or mod == '/': # bug in data entry\n",
    "                            i1 -= 1\n",
    "                            continue\n",
    "                        \n",
    "                        modifiers[-1].append(mod)\n",
    "                        if i1 > 1:\n",
    "                            if splits[i1-1] == ' ':\n",
    "                                # compound modifier\n",
    "                                pass\n",
    "                                \n",
    "                            elif splits[i1-1] == '/':\n",
    "                                # end of current modifier\n",
    "                                next_token = splits[i1-2]\n",
    "                                if next_token in super_genres:\n",
    "                                    print('next token is a super genre')\n",
    "                                    break\n",
    "                                else:\n",
    "                                    modifiers.append([])\n",
    "                                \n",
    "                            else:\n",
    "                                raise RuntimeError('bug')\n",
    "                        else:\n",
    "                            # Just one modifier and we're done\n",
    "                            break\n",
    "                        i1 -= 2\n",
    "                    \n",
    "                    # reverse order of modifiers to match up with text\n",
    "                    modifiers = [[m for m in reversed(modifier)] for modifier in reversed(modifiers)]\n",
    "                    \n",
    "                    tokens.append([*modifiers, token])\n",
    "                    splitter(tokens, splits, i1-2)\n",
    "                    \n",
    "                elif splits[i-1] == '/':\n",
    "                    # token is a bare super genre\n",
    "                    tokens.append(token)\n",
    "                    splitter(tokens, splits, i-2)\n",
    "                \n",
    "                else:\n",
    "                    raise RuntimeError()\n",
    "            elif i == 0:\n",
    "                # token is a bare super genre and we're done\n",
    "                tokens.append(token)\n",
    "        \n",
    "        tokens = []\n",
    "        splitter(tokens, splits, len(splits)-1)\n",
    "        print('tokens =', tokens)\n",
    "            \n",
    "\n",
    "            \n",
    "        return []\n",
    "    \n",
    "    \n",
    "tokenizer = GenreTokenizer()\n",
    "genres_tokens = [(genre_text, tokenizer.tokenize(genre_text)) for genre_text in unique_genres]\n",
    "#pprint(genres_tokens[0:25])\n",
    "unique_tokens = {token for tup in genres_tokens for token in tup[1]}\n",
    "print(len(unique_tokens))\n",
    "pprint(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common genres and modifiers?\n",
    "These are the most common words... that should allow me to pick them out manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "word_counts = collections.defaultdict(int)\n",
    "split_pattern = re.compile(r'[/\\s]+')\n",
    "for genre_text in unique_genres:\n",
    "    genre_text = tokenizer.normalizeWhitespace(genre_text).lower()\n",
    "    genre_text = tokenizer.removeParens(genre_text)\n",
    "    texts = genre_text.split(',')\n",
    "\n",
    "    for text in texts:\n",
    "        for word in re.split(split_pattern, text):\n",
    "            if not word:\n",
    "                continue\n",
    "            \n",
    "            word_counts[word] += 1\n",
    "\n",
    "pprint(sorted(word_counts.items(), key=lambda t: t[1]))\n",
    "super_genres = frozenset([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
